{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDsVMGiVgSq2"
      },
      "source": [
        "## Классификация FashionMNIST\n",
        "\n",
        "##### Автор: [Радослав Нейчев](https://www.linkedin.com/in/radoslav-neychev/), https://t.me/s/girafe_ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3isBRG6PgSq6"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from IPython.display import clear_output\n",
        "from matplotlib import pyplot as plt\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torchvision.datasets import FashionMNIST\n",
        "\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4T641xbYeExp"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "def get_predictions(model, eval_data, step=10):\n",
        "\n",
        "    predicted_labels = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for idx in range(0, len(eval_data), step):\n",
        "            y_predicted = model(eval_data[idx : idx + step].to(device))\n",
        "            predicted_labels.append(y_predicted.argmax(dim=1).cpu())\n",
        "\n",
        "    predicted_labels = torch.cat(predicted_labels)\n",
        "    predicted_labels = \",\".join([str(x.item()) for x in list(predicted_labels)])\n",
        "    return predicted_labels\n",
        "\n",
        "\n",
        "def get_accuracy(model, data_loader):\n",
        "    predicted_labels = []\n",
        "    real_labels = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            y_predicted = model(batch[0].to(device))\n",
        "            predicted_labels.append(y_predicted.argmax(dim=1).cpu())\n",
        "            real_labels.append(batch[1])\n",
        "\n",
        "    predicted_labels = torch.cat(predicted_labels)\n",
        "    real_labels = torch.cat(real_labels)\n",
        "    accuracy_score = (predicted_labels == real_labels).type(torch.FloatTensor).mean()\n",
        "    return accuracy_score\n",
        "\n",
        "\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsKZEzQ_eExv"
      },
      "source": [
        "Загрузите файл `hw_overfitting_data_dict.npy` (ссылка есть на странице с заданием), он понадобится для генерации посылок. Код ниже может его загрузить (но в случае возникновения ошибки скачайте и загрузите его вручную).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sOvbD96yeExx",
        "outputId": "9c9948f9-f118-4e8d-d5f2-bc7f73e49f8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-12 10:08:52--  https://github.com/girafe-ai/ml-course/raw/24f_ysda/homeworks/hw_overfitting/hw_overfitting_data_dict\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/girafe-ai/ml-course/24f_ysda/homeworks/hw_overfitting/hw_overfitting_data_dict [following]\n",
            "--2025-04-12 10:08:52--  https://raw.githubusercontent.com/girafe-ai/ml-course/24f_ysda/homeworks/hw_overfitting/hw_overfitting_data_dict\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6272446 (6.0M) [application/octet-stream]\n",
            "Saving to: ‘hw_overfitting_data_dict.npy’\n",
            "\n",
            "hw_overfitting_data 100%[===================>]   5.98M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-04-12 10:08:52 (234 MB/s) - ‘hw_overfitting_data_dict.npy’ saved [6272446/6272446]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/girafe-ai/ml-course/raw/24f_ysda/homeworks/hw_overfitting/hw_overfitting_data_dict -O hw_overfitting_data_dict.npy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pPCvC_gseEx0"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "assert os.path.exists(\n",
        "    \"hw_overfitting_data_dict.npy\"\n",
        "), \"Please, download `hw_overfitting_data_dict.npy` and place it in the working directory\"\n",
        "\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeA6Q5-CgSq7"
      },
      "source": [
        "Вернемся к задаче распознавания простых изображений, рассмотренной ранее. Но теперь будем работать с набором данных [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist). В данном задании воспользуемся всем датасетом целиком.\n",
        "\n",
        "__Ваша первая задача: реализовать весь пайплан обучения модели и добиться качества $\\geq 88.5\\%$ на тестовой выборке.__\n",
        "\n",
        "Код для обучения модели в данном задании отсутствует. Присутствует лишь несколько тестов, которые помогут вам отладить свое решение. За примером можно обратиться к ноутбукам с предыдущих занятий."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QLrebdEIeEx8"
      },
      "outputs": [],
      "source": [
        "CUDA_DEVICE_ID = 0  # change if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nPG1KbQAgl8b"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "device = (\n",
        "    torch.device(f\"cuda:{CUDA_DEVICE_ID}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        ")\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "aYcL28OsgSq8",
        "outputId": "93399d28-4167-4cba-a740-2695e522cf67"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Image label: 0')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "\n",
        "train_fmnist_data = FashionMNIST(\n",
        "    \".\", train=True, transform=torchvision.transforms.ToTensor(), download=True\n",
        ")\n",
        "test_fmnist_data = FashionMNIST(\n",
        "    \".\", train=False, transform=torchvision.transforms.ToTensor(), download=True\n",
        ")\n",
        "\n",
        "\n",
        "train_data_loader = torch.utils.data.DataLoader(\n",
        "    train_fmnist_data, batch_size=32, shuffle=True, num_workers=2\n",
        ")\n",
        "\n",
        "test_data_loader = torch.utils.data.DataLoader(\n",
        "    test_fmnist_data, batch_size=32, shuffle=False, num_workers=2\n",
        ")\n",
        "\n",
        "random_batch = next(iter(train_data_loader))\n",
        "_image, _label = random_batch[0][0], random_batch[1][0]\n",
        "plt.figure()\n",
        "plt.imshow(_image.reshape(28, 28))\n",
        "plt.title(f\"Image label: {_label}\")\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6jWRv1rgSq8"
      },
      "source": [
        "Постройте модель ниже. Пожалуйста, не стройте переусложненную сеть, не стоит делать ее глубже четырех слоев (можно и меньше). Ваша основная задача – обучить модель и получить качество на отложенной (тестовой выборке) не менее 88.5% accuracy.\n",
        "\n",
        "__Внимание, ваша модель должна быть представлена именно переменной `model_task_1`. На вход ей должен приходить тензор размерностью (1, 28, 28).__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "BcyEFX-RgSq8"
      },
      "outputs": [],
      "source": [
        "# Creating model instance\n",
        "class FMNIST(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FMNIST, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # Added padding\n",
        "        self.bn1 = nn.BatchNorm2d(32) # Batch Normalization\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # Added padding\n",
        "        self.bn2 = nn.BatchNorm2d(64) # Batch Normalization\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.25) # Dropout\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
        "        self.bn3 = nn.BatchNorm1d(512) # Batch Normalization\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        x = self.dropout(x) # Apply Dropout\n",
        "        x = F.relu(self.bn3(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "model_task_1 = FMNIST()\n",
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAoLV4dkoy5M"
      },
      "source": [
        "Не забудьте перенести модель на выбранный `device`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Xas9SIXDoxvZ",
        "outputId": "2f903f5a-b433-4b30-983b-0a269b087fe1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FMNIST(\n",
              "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (dropout): Dropout(p=0.25, inplace=False)\n",
              "  (fc1): Linear(in_features=3136, out_features=512, bias=True)\n",
              "  (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "model_task_1.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pLRWysggSq9"
      },
      "source": [
        "Локальные тесты для проверки вашей модели доступны ниже:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qMQzo1ggSq9",
        "outputId": "5aec3f68-3af9-4d83-e7ab-a38350e5501c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Everything seems fine!\n"
          ]
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "assert model_task_1 is not None, \"Please, use `model_task_1` variable to store your model\"\n",
        "\n",
        "try:\n",
        "    x = random_batch[0].to(device)\n",
        "    y = random_batch[1].to(device)\n",
        "\n",
        "    # compute outputs given inputs, both are variables\n",
        "    y_predicted = model_task_1(x)\n",
        "except Exception as e:\n",
        "    print(\"Something is wrong with the model\")\n",
        "    raise e\n",
        "\n",
        "\n",
        "assert y_predicted.shape[-1] == 10, \"Model should predict 10 logits/probas\"\n",
        "\n",
        "print(\"Everything seems fine!\")\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suRmIPwIgSq9"
      },
      "source": [
        "Настройте параметры модели на обучающей выборке. Также рекомендуем поработать с `learning rate`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "YJnU14bdnZa_",
        "outputId": "0335baa9-1dd8-4045-e700-7341cef4cfac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   200] loss: 0.743\n",
            "[1,   400] loss: 0.476\n",
            "[1,   600] loss: 0.427\n",
            "[1,   800] loss: 0.386\n",
            "[1,  1000] loss: 0.365\n",
            "[1,  1200] loss: 0.361\n",
            "[1,  1400] loss: 0.340\n",
            "[1,  1600] loss: 0.344\n",
            "[1,  1800] loss: 0.300\n",
            "[2,   200] loss: 0.292\n",
            "[2,   400] loss: 0.285\n",
            "[2,   600] loss: 0.289\n",
            "[2,   800] loss: 0.267\n",
            "[2,  1000] loss: 0.285\n",
            "[2,  1200] loss: 0.288\n",
            "[2,  1400] loss: 0.281\n",
            "[2,  1600] loss: 0.283\n",
            "[2,  1800] loss: 0.283\n",
            "[3,   200] loss: 0.240\n",
            "[3,   400] loss: 0.241\n",
            "[3,   600] loss: 0.241\n",
            "[3,   800] loss: 0.243\n",
            "[3,  1000] loss: 0.239\n",
            "[3,  1200] loss: 0.251\n",
            "[3,  1400] loss: 0.254\n",
            "[3,  1600] loss: 0.246\n",
            "[3,  1800] loss: 0.248\n",
            "[4,   200] loss: 0.210\n",
            "[4,   400] loss: 0.217\n",
            "[4,   600] loss: 0.220\n",
            "[4,   800] loss: 0.218\n",
            "[4,  1000] loss: 0.221\n",
            "[4,  1200] loss: 0.233\n",
            "[4,  1400] loss: 0.225\n",
            "[4,  1600] loss: 0.226\n",
            "[4,  1800] loss: 0.214\n",
            "[5,   200] loss: 0.197\n",
            "[5,   400] loss: 0.200\n",
            "[5,   600] loss: 0.192\n",
            "[5,   800] loss: 0.204\n",
            "[5,  1000] loss: 0.196\n",
            "[5,  1200] loss: 0.201\n",
            "[5,  1400] loss: 0.196\n",
            "[5,  1600] loss: 0.191\n",
            "[5,  1800] loss: 0.193\n",
            "[6,   200] loss: 0.174\n",
            "[6,   400] loss: 0.174\n",
            "[6,   600] loss: 0.186\n",
            "[6,   800] loss: 0.177\n",
            "[6,  1000] loss: 0.183\n",
            "[6,  1200] loss: 0.184\n",
            "[6,  1400] loss: 0.184\n",
            "[6,  1600] loss: 0.176\n",
            "[6,  1800] loss: 0.183\n",
            "[7,   200] loss: 0.167\n",
            "[7,   400] loss: 0.162\n",
            "[7,   600] loss: 0.167\n",
            "[7,   800] loss: 0.167\n",
            "[7,  1000] loss: 0.160\n",
            "[7,  1200] loss: 0.161\n",
            "[7,  1400] loss: 0.166\n",
            "[7,  1600] loss: 0.177\n",
            "[7,  1800] loss: 0.171\n",
            "[8,   200] loss: 0.142\n",
            "[8,   400] loss: 0.152\n",
            "[8,   600] loss: 0.154\n",
            "[8,   800] loss: 0.142\n",
            "[8,  1000] loss: 0.152\n",
            "[8,  1200] loss: 0.163\n",
            "[8,  1400] loss: 0.153\n",
            "[8,  1600] loss: 0.161\n",
            "[8,  1800] loss: 0.158\n",
            "[9,   200] loss: 0.145\n",
            "[9,   400] loss: 0.147\n",
            "[9,   600] loss: 0.137\n",
            "[9,   800] loss: 0.136\n",
            "[9,  1000] loss: 0.141\n",
            "[9,  1200] loss: 0.142\n",
            "[9,  1400] loss: 0.139\n",
            "[9,  1600] loss: 0.145\n",
            "[9,  1800] loss: 0.147\n",
            "[10,   200] loss: 0.131\n",
            "[10,   400] loss: 0.132\n",
            "[10,   600] loss: 0.129\n",
            "[10,   800] loss: 0.122\n",
            "[10,  1000] loss: 0.135\n",
            "[10,  1200] loss: 0.137\n",
            "[10,  1400] loss: 0.127\n",
            "[10,  1600] loss: 0.129\n",
            "[10,  1800] loss: 0.132\n",
            "[11,   200] loss: 0.119\n",
            "[11,   400] loss: 0.125\n",
            "[11,   600] loss: 0.127\n",
            "[11,   800] loss: 0.128\n",
            "[11,  1000] loss: 0.130\n",
            "[11,  1200] loss: 0.116\n",
            "[11,  1400] loss: 0.111\n",
            "[11,  1600] loss: 0.130\n",
            "[11,  1800] loss: 0.139\n",
            "[12,   200] loss: 0.111\n",
            "[12,   400] loss: 0.111\n",
            "[12,   600] loss: 0.116\n",
            "[12,   800] loss: 0.121\n",
            "[12,  1000] loss: 0.105\n",
            "[12,  1200] loss: 0.117\n",
            "[12,  1400] loss: 0.114\n",
            "[12,  1600] loss: 0.122\n",
            "[12,  1800] loss: 0.117\n",
            "[13,   200] loss: 0.103\n",
            "[13,   400] loss: 0.103\n",
            "[13,   600] loss: 0.102\n",
            "[13,   800] loss: 0.102\n",
            "[13,  1000] loss: 0.103\n",
            "[13,  1200] loss: 0.108\n",
            "[13,  1400] loss: 0.109\n",
            "[13,  1600] loss: 0.114\n",
            "[13,  1800] loss: 0.105\n",
            "[14,   200] loss: 0.088\n",
            "[14,   400] loss: 0.093\n",
            "[14,   600] loss: 0.091\n",
            "[14,   800] loss: 0.097\n",
            "[14,  1000] loss: 0.097\n",
            "[14,  1200] loss: 0.096\n",
            "[14,  1400] loss: 0.100\n",
            "[14,  1600] loss: 0.100\n",
            "[14,  1800] loss: 0.106\n",
            "[15,   200] loss: 0.079\n",
            "[15,   400] loss: 0.084\n",
            "[15,   600] loss: 0.086\n",
            "[15,   800] loss: 0.080\n",
            "[15,  1000] loss: 0.086\n",
            "[15,  1200] loss: 0.088\n",
            "[15,  1400] loss: 0.096\n",
            "[15,  1600] loss: 0.104\n",
            "[15,  1800] loss: 0.095\n",
            "[16,   200] loss: 0.076\n",
            "[16,   400] loss: 0.076\n",
            "[16,   600] loss: 0.088\n",
            "[16,   800] loss: 0.082\n",
            "[16,  1000] loss: 0.082\n",
            "[16,  1200] loss: 0.081\n",
            "[16,  1400] loss: 0.077\n",
            "[16,  1600] loss: 0.091\n",
            "[16,  1800] loss: 0.088\n",
            "[17,   200] loss: 0.074\n",
            "[17,   400] loss: 0.069\n",
            "[17,   600] loss: 0.075\n",
            "[17,   800] loss: 0.079\n",
            "[17,  1000] loss: 0.077\n",
            "[17,  1200] loss: 0.076\n",
            "[17,  1400] loss: 0.075\n",
            "[17,  1600] loss: 0.082\n",
            "[17,  1800] loss: 0.080\n",
            "[18,   200] loss: 0.072\n",
            "[18,   400] loss: 0.063\n",
            "[18,   600] loss: 0.068\n",
            "[18,   800] loss: 0.070\n",
            "[18,  1000] loss: 0.069\n",
            "[18,  1200] loss: 0.070\n",
            "[18,  1400] loss: 0.072\n",
            "[18,  1600] loss: 0.072\n",
            "[18,  1800] loss: 0.069\n",
            "[19,   200] loss: 0.063\n",
            "[19,   400] loss: 0.064\n",
            "[19,   600] loss: 0.062\n",
            "[19,   800] loss: 0.063\n",
            "[19,  1000] loss: 0.059\n",
            "[19,  1200] loss: 0.070\n",
            "[19,  1400] loss: 0.072\n",
            "[19,  1600] loss: 0.065\n",
            "[19,  1800] loss: 0.076\n",
            "[20,   200] loss: 0.052\n",
            "[20,   400] loss: 0.054\n",
            "[20,   600] loss: 0.057\n",
            "[20,   800] loss: 0.064\n",
            "[20,  1000] loss: 0.057\n",
            "[20,  1200] loss: 0.060\n",
            "[20,  1400] loss: 0.067\n",
            "[20,  1600] loss: 0.064\n",
            "[20,  1800] loss: 0.061\n",
            "[21,   200] loss: 0.053\n",
            "[21,   400] loss: 0.053\n",
            "[21,   600] loss: 0.057\n",
            "[21,   800] loss: 0.052\n",
            "[21,  1000] loss: 0.057\n",
            "[21,  1200] loss: 0.060\n",
            "[21,  1400] loss: 0.058\n",
            "[21,  1600] loss: 0.059\n",
            "[21,  1800] loss: 0.055\n",
            "[22,   200] loss: 0.052\n",
            "[22,   400] loss: 0.047\n",
            "[22,   600] loss: 0.047\n",
            "[22,   800] loss: 0.052\n",
            "[22,  1000] loss: 0.055\n",
            "[22,  1200] loss: 0.055\n",
            "[22,  1400] loss: 0.055\n",
            "[22,  1600] loss: 0.055\n",
            "[22,  1800] loss: 0.061\n",
            "[23,   200] loss: 0.042\n",
            "[23,   400] loss: 0.048\n",
            "[23,   600] loss: 0.048\n",
            "[23,   800] loss: 0.043\n",
            "[23,  1000] loss: 0.048\n",
            "[23,  1200] loss: 0.050\n",
            "[23,  1400] loss: 0.049\n",
            "[23,  1600] loss: 0.052\n",
            "[23,  1800] loss: 0.049\n",
            "[24,   200] loss: 0.043\n",
            "[24,   400] loss: 0.041\n",
            "[24,   600] loss: 0.049\n",
            "[24,   800] loss: 0.042\n",
            "[24,  1000] loss: 0.043\n",
            "[24,  1200] loss: 0.042\n",
            "[24,  1400] loss: 0.045\n",
            "[24,  1600] loss: 0.041\n",
            "[24,  1800] loss: 0.049\n",
            "[25,   200] loss: 0.045\n",
            "[25,   400] loss: 0.039\n",
            "[25,   600] loss: 0.039\n",
            "[25,   800] loss: 0.044\n",
            "[25,  1000] loss: 0.039\n",
            "[25,  1200] loss: 0.041\n",
            "[25,  1400] loss: 0.048\n",
            "[25,  1600] loss: 0.043\n",
            "[25,  1800] loss: 0.046\n",
            "[26,   200] loss: 0.037\n",
            "[26,   400] loss: 0.045\n",
            "[26,   600] loss: 0.039\n",
            "[26,   800] loss: 0.042\n",
            "[26,  1000] loss: 0.041\n",
            "[26,  1200] loss: 0.043\n",
            "[26,  1400] loss: 0.043\n",
            "[26,  1600] loss: 0.040\n",
            "[26,  1800] loss: 0.043\n",
            "[27,   200] loss: 0.031\n",
            "[27,   400] loss: 0.035\n",
            "[27,   600] loss: 0.039\n",
            "[27,   800] loss: 0.035\n",
            "[27,  1000] loss: 0.040\n",
            "[27,  1200] loss: 0.042\n",
            "[27,  1400] loss: 0.036\n",
            "[27,  1600] loss: 0.039\n",
            "[27,  1800] loss: 0.036\n",
            "[28,   200] loss: 0.036\n",
            "[28,   400] loss: 0.031\n",
            "[28,   600] loss: 0.037\n",
            "[28,   800] loss: 0.034\n",
            "[28,  1000] loss: 0.038\n",
            "[28,  1200] loss: 0.034\n",
            "[28,  1400] loss: 0.039\n",
            "[28,  1600] loss: 0.040\n",
            "[28,  1800] loss: 0.037\n",
            "[29,   200] loss: 0.029\n",
            "[29,   400] loss: 0.029\n",
            "[29,   600] loss: 0.027\n",
            "[29,   800] loss: 0.034\n",
            "[29,  1000] loss: 0.033\n",
            "[29,  1200] loss: 0.032\n",
            "[29,  1400] loss: 0.034\n",
            "[29,  1600] loss: 0.032\n",
            "[29,  1800] loss: 0.034\n",
            "[30,   200] loss: 0.029\n",
            "[30,   400] loss: 0.031\n",
            "[30,   600] loss: 0.028\n",
            "[30,   800] loss: 0.028\n",
            "[30,  1000] loss: 0.032\n",
            "[30,  1200] loss: 0.034\n",
            "[30,  1400] loss: 0.035\n",
            "[30,  1600] loss: 0.033\n",
            "[30,  1800] loss: 0.033\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 91.72%\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_task_1.parameters(), lr=0.0001)\n",
        "\n",
        "epochs = 30\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_data_loader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_task_1(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_data_loader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = model_task_1(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy of the network on the 10000 test images: {accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zce7gt1gSq-"
      },
      "source": [
        "Также, напоминаем, что в любой момент можно обратиться к замечательной [документации](https://pytorch.org/docs/stable/index.html) и [обучающим примерам](https://pytorch.org/tutorials/).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usswrWYOgSq-"
      },
      "source": [
        "Оценим качество классификации:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Xua3TVZHgSq-",
        "outputId": "5e2c2a9b-2a64-4d82-c74f-7e1a72d091af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural network accuracy on train set: 0.99908\n"
          ]
        }
      ],
      "source": [
        "train_acc_task_1 = get_accuracy(model_task_1, train_data_loader)\n",
        "print(f\"Neural network accuracy on train set: {train_acc_task_1:3.5}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "l9KEKXBxgSq-",
        "outputId": "38c97ce0-d4e6-4c3a-b277-e140932521ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural network accuracy on test set: 0.9303\n"
          ]
        }
      ],
      "source": [
        "test_acc_task_1 = get_accuracy(model_task_1, test_data_loader)\n",
        "print(f\"Neural network accuracy on test set: {test_acc_task_1:3.5}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oyhmMobgSq_"
      },
      "source": [
        "Проверка, что необходимые пороги пройдены:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "OAIrURCEgSq_"
      },
      "outputs": [],
      "source": [
        "assert test_acc_task_1 >= 0.885, \"Train accuracy is below 0.885 threshold\"\n",
        "assert (\n",
        "    train_acc_task_1 >= 0.905\n",
        "), \"Train accuracy is below 0.905 while test accuracy is fine. We recommend to check your model and data flow\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d89A32eceEyn"
      },
      "source": [
        "Обращаем внимане, код ниже предполагает, что ваша модель имеет содержится в переменной `model_task_1`, а файл `hw_fmnist_data_dict.npy` находится в той же директории, что и ноутбук (он доступен в репозитории)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "2h3CgIMueEyo",
        "outputId": "fd677a41-8558-4fac-f7b9-b528beabcdad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File saved to `submission_dict_fmnist_task_1.json`\n"
          ]
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "assert os.path.exists(\n",
        "    \"hw_fmnist_data_dict.npy\"\n",
        "), \"Please, download `hw_fmnist_data_dict.npy` and place it in the working directory\"\n",
        "\n",
        "loaded_data_dict = np.load(\"/content/hw_overfitting_data_dict.npy\", allow_pickle=True)\n",
        "\n",
        "submission_dict = {\n",
        "    \"train_predictions_task_1\": get_predictions(\n",
        "        model_task_1, torch.FloatTensor(loaded_data_dict.item()[\"train\"])\n",
        "    ),\n",
        "    \"test_predictions_task_1\": get_predictions(\n",
        "        model_task_1, torch.FloatTensor(loaded_data_dict.item()[\"test\"])\n",
        "    ),\n",
        "}\n",
        "\n",
        "with open(\"submission_dict_fmnist_task_1.json\", \"w\") as iofile:\n",
        "    json.dump(submission_dict, iofile)\n",
        "print(\"File saved to `submission_dict_fmnist_task_1.json`\")\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_7KLMs_eEyo"
      },
      "source": [
        "### Сдача задания\n",
        "Сдайте сгенерированный файл в соответствующую задачу в соревновании, а именно:\n",
        "    \n",
        "* `submission_dict_fmnist_task_1.json` в задачу Separation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtWnYAN_gSrA"
      },
      "source": [
        "На этом задание завершено. Поздравляем!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "21499ab2a6726e29f7050b76af0e9680227e613293d630ba279de7ebdfad9cae"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}